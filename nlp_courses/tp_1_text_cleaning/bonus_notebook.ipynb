{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\antoi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re  # noqa: F401\n",
    "import string  # noqa: F401\n",
    "\n",
    "import nltk  # noqa: F401\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, wordnet  # noqa: F401\n",
    "from nltk.stem import WordNetLemmatizer  # noqa: F401\n",
    "from sklearn.pipeline import Pipeline  # noqa: F401\n",
    "from sklearn.preprocessing import FunctionTransformer  # noqa: F401\n",
    "from utils import emojis_unicode, emoticons, slang_words  # noqa: F401\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare your cleaning functions here\n",
    "# Chain those functions together inside the preprocessing pipeline\n",
    "# You can use (or not) Sklearn pipelines and functionTransformer for readability\n",
    "# and modularity\n",
    "# --- Documentation ---\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from the input text.\n",
    "    \"\"\"\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    translation_table = str.maketrans('', '', PUNCT_TO_REMOVE)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "def remove_stopwords(text: str,language: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input text.\n",
    "    \"\"\"\n",
    "    STOPWORDS = set(stopwords.words(language))\n",
    "    split = text.split()\n",
    "    filtered_words = [word for word in split if word not in STOPWORDS]\n",
    "    return \" \".join(filtered_words)\n",
    "'''\n",
    "def remove_frequent_words(text: str, freq_words: list) -> str:\n",
    "    \"\"\"\n",
    "    Removes frequent words from the input text.\n",
    "    \"\"\"\n",
    "    most_common = Counter(\" \".join(text_df[\"text_wo_stop\"]).split()).most_common()\n",
    "    FREQWORDS = [w for (w, word_count) in most_common[:10]]\n",
    "    split = text.split()\n",
    "    filtered_words = [word for word in split if word not in freq_words]\n",
    "    return \" \".join(filtered_words)\n",
    "    \n",
    "Here I found out it was difficult to remove words on the whole dataset \n",
    "Because the apply function is applying per row not on the whole\n",
    "Making it difficult to remove the most common words\n",
    "\n",
    "def remove_rare_words(text: str, rare_words: list) -> str:\n",
    "    \"\"\"\n",
    "    Removes rare words from the input text.\n",
    "    \"\"\"\n",
    "    split = text.split()\n",
    "    filtered_words = [word for word in split if word not in rare_words]\n",
    "    return \" \".join(filtered_words)\n",
    "'''\n",
    "def stemming(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Applies stemming to words in the input text.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    split = text.split()\n",
    "    filtered_words = [stemmer.stem(word) for word in split]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatizes words in the input text.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "    pos_tagged_text = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "def convert_emoticons(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts emoticons to text in the input text.\n",
    "    \"\"\"\n",
    "    EMOTICONS = emoticons()\n",
    "    for emoticon, description in EMOTICONS.items():\n",
    "        text = re.sub(emoticon, \"_\".join(description.replace(\",\", \"\").split()), text)\n",
    "    return text\n",
    "\n",
    "def convert_emojis(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts emojis to text in the input text.\n",
    "    \"\"\"\n",
    "    EMO_UNICODE = emojis_unicode()\n",
    "    #UNICODE_EMO = {v: k for k, v in EMO_UNICODE.items()}\n",
    "    for description, emoji in EMO_UNICODE.items():\n",
    "        text = text.replace(emoji, \"_\".join(description.replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes URLs from the input text.\n",
    "    \"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_http_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes HTTP tags from the input text.\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(text, \"html.parser\").text\n",
    "\n",
    "def chat_words_conversion(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts chat words to standard words in the input text.\n",
    "    \"\"\"\n",
    "    slang_words_list = slang_words()\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        new_text.append(slang_words_list.get(word.upper(), word))\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def spell_correction(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Corrects spelling errors in the input text.\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_word = spell.correction(word)\n",
    "            corrected_text.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we test our functions one by one\n",
    "assert lower_case(\"Hello World!\") == \"hello world!\"\n",
    "assert remove_punctuation(\"Hello, World!\") == \"Hello World\"\n",
    "assert remove_stopwords(\"Hello the World!\", 'english') == \"Hello World!\"\n",
    "assert stemming(\"console consoling\") == \"consol consol\"\n",
    "assert lemmatize(\"feet caring\") == \"foot care\"\n",
    "assert convert_emoticons(\"I am sad :(\") == \"I am sad Frown_sad_andry_or_pouting\"\n",
    "assert convert_emojis(\"game is on üî•\") == \"game is on fire\"\n",
    "assert remove_urls(\"https://www.google.com\") == \"\"\n",
    "assert remove_http_tags(\"<p>hello world</p>\") == \"hello world\"\n",
    "assert chat_words_conversion(\"one minute BRB\") == \"one minute Be Right Back\"\n",
    "assert (spell_correction(\"THISNOTAWORD Hopefully you larned smething durng th classn, seeee you in twwo wekks !\")) == \"THISNOTAWORD Hopefully you learned something during the class see you in two weeks !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Chains all the cleaning functions together using scikit-learn pipelines.\n",
    "    \"\"\"\n",
    "    # Define your global variables (e.g., STOPWORDS, FREQWORDS, RAREWORDS) here\n",
    "\n",
    "    # Define the preprocessing steps as a list of tuples with (step_name, transformer_function)\n",
    "    preprocessing_steps = [\n",
    "        ('lower_case', FunctionTransformer(lower_case)),\n",
    "        ('remove_urls', FunctionTransformer(remove_urls)),\n",
    "        ('remove_http_tags', FunctionTransformer(remove_http_tags)),\n",
    "        ('remove_punctuation', FunctionTransformer(remove_punctuation)),\n",
    "        ('remove_stopwords', FunctionTransformer(lambda x: remove_stopwords(x, 'english'))),\n",
    "        #('remove_frequent_words', FunctionTransformer(lambda x: remove_frequent_words(x))),\n",
    "        #('remove_rare_words', FunctionTransformer(lambda x: remove_rare_words(x))),\n",
    "        # stemming is less efficient than lemmatization so we are not using it here\n",
    "        # ('stemming', FunctionTransformer(lambda x: stemming(x))),  # Replace 'stemmer' with your stemmer object\n",
    "        ('lemmatize', FunctionTransformer(lambda x: lemmatize(x))),  # Replace 'lemmatizer' with your lemmatizer object\n",
    "        ('convert_emoticons', FunctionTransformer(lambda x: convert_emoticons(x))),\n",
    "        ('convert_emojis', FunctionTransformer(lambda x: convert_emojis(x))),\n",
    "        ('chat_words_conversion', FunctionTransformer(lambda x: chat_words_conversion(x))),  # Replace 'slang_words_list' with your dictionary\n",
    "        ('spell_correction', FunctionTransformer(lambda x: spell_correction(x)))  # Replace 'spell' with your SpellChecker object\n",
    "    ]\n",
    "\n",
    "    # Create the pipeline\n",
    "    preprocessing_pipeline = Pipeline(preprocessing_steps)\n",
    "\n",
    "    # Apply the pipeline to the input text\n",
    "    cleaned_text = preprocessing_pipeline.transform([text][0])\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base text: Hello Amazon - my package never arrived :( https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first PLEASE FIX ASAP ‚è∞! @AmazonHelp <test/>\n",
      "Cleaned text: hello amazon package never arrive please fix As Soon As Possible alarm_clock amazonhelp\n",
      "\n",
      "\n",
      "Base text: Hello! üòä This is an example text with emojis! üëç\n",
      "Cleaned text: hello smiling_face_with_smiling_eyes example text emojis thumbs_up\n",
      "\n",
      "\n",
      "Base text: <p>This is a <b>sample</b> text with <a href='https://www.example.com'>HTML</a> tags.</p>\n",
      "Cleaned text: sample text ref tags\n",
      "\n",
      "\n",
      "Base text: The quick brown fox jumps over the lazy dog.\n",
      "Cleaned text: quick brown fox jump lazy dog\n",
      "\n",
      "\n",
      "Base text: Visit our website at https://www.example.com for more information\n",
      "Cleaned text: visit website information\n",
      "\n",
      "\n",
      "Base text: I'm feeling üòÑ today. Don't worry üòâ.\n",
      "Cleaned text: im feel smiling_face_with_open_mouth_&_smiling_eyes today dont worry winking_face\n",
      "\n",
      "\n",
      "Base text: This text contains special characters #$%&@*!\n",
      "Cleaned text: text contain special character\n",
      "\n",
      "\n",
      "Base text: LOL BRB and OMG are common chat abbreviations.\n",
      "Cleaned text: Laughing Out Loud Be Right Back Oh My God common chat abbreviation\n",
      "\n",
      "\n",
      "Base text: üòÇüòçüëè Just saw the funniest movie ever! üòÇüòçüëè\n",
      "Cleaned text: face_with_tears_of_joysmiling_face_with_heart-eyesclapping_hands saw funny movie ever face_with_tears_of_joysmiling_face_with_heart-eyesclapping_hands\n",
      "\n",
      "\n",
      "Base text: <a href='https://www.example.com'>Click here</a> for more info\n",
      "Cleaned text: ref here info\n",
      "\n",
      "\n",
      "Base text: I found a great recipe at https://www.recipes.com! üòã It's so delicious! #cooking\n",
      "Cleaned text: find great recipe face_savouring_delicious_food delicious cooking\n",
      "\n",
      "\n",
      "Base text: M8 imho this NLP thing is kinda üî• !\n",
      "Cleaned text: Mate In My Humble Opinion nap thing kinda fire\n",
      "\n",
      "\n",
      "Base text: Hate Is A Place Where A Man Who Can‚Äôt Stand Sadness Goes. üòû\n",
      "Cleaned text: hate place man can i i stand sadness go disappointed_face\n",
      "\n",
      "\n",
      "Base text: This Sword Is The Proof That I Have Lived. üóø\n",
      "Cleaned text: sword proof live moai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"to_clean.csv\", index_col=0)\n",
    "    df[\"cleaned_text\"] = df.text.apply(lambda x: preprocessing_pipeline(x))\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\nBase text: {row.text}\")\n",
    "        print(f\"Cleaned text: {row.cleaned_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
