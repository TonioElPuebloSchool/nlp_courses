{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # noqa: F401\n",
    "import string  # noqa: F401\n",
    "\n",
    "import nltk  # noqa: F401\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, wordnet  # noqa: F401\n",
    "from nltk.stem import WordNetLemmatizer  # noqa: F401\n",
    "from sklearn.pipeline import Pipeline  # noqa: F401\n",
    "from sklearn.preprocessing import FunctionTransformer  # noqa: F401\n",
    "from utils import emojis_unicode, emoticons, slang_words  # noqa: F401\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from spellchecker import SpellChecker\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare your cleaning functions here\n",
    "# Chain those functions together inside the preprocessing pipeline\n",
    "# You can use (or not) Sklearn pipelines and functionTransformer for readability\n",
    "# and modularity\n",
    "# --- Documentation ---\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts the input text to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes punctuation from the input text.\n",
    "    \"\"\"\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    translation_table = str.maketrans('', '', PUNCT_TO_REMOVE)\n",
    "    return text.translate(translation_table)\n",
    "\n",
    "def remove_stopwords(text: str,language: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes stopwords from the input text.\n",
    "    \"\"\"\n",
    "    STOPWORDS = set(stopwords.words(language))\n",
    "    split = text.split()\n",
    "    filtered_words = [word for word in split if word not in STOPWORDS]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_frequent_words(text: str, freq_words: list) -> str:\n",
    "    \"\"\"\n",
    "    Removes frequent words from the input text.\n",
    "    \"\"\"\n",
    "    split = text.split()\n",
    "    filtered_words = [word for word in split if word not in freq_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_rare_words(text: str, rare_words: list) -> str:\n",
    "    \"\"\"\n",
    "    Removes rare words from the input text.\n",
    "    \"\"\"\n",
    "    split = text.split()\n",
    "    filtered_words = [word for word in split if word not in rare_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def stemming(text: str, stemmer) -> str:\n",
    "    \"\"\"\n",
    "    Applies stemming to words in the input text.\n",
    "    \"\"\"\n",
    "    split = text.split()\n",
    "    filtered_words = [stemmer.stem(word) for word in split]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def lemmatize(text: str, lemmatizer) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatizes words in the input text.\n",
    "    \"\"\"\n",
    "    wordnet_map = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }\n",
    "    pos_tagged_text = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "def convert_emoticons(text: str, EMOTICONS: dict) -> str:\n",
    "    \"\"\"\n",
    "    Converts emoticons to text in the input text.\n",
    "    \"\"\"\n",
    "    for emoticon, description in EMOTICONS.items():\n",
    "        text = re.sub(emoticon, \"_\".join(description.replace(\",\", \"\").split()), text)\n",
    "    return text\n",
    "\n",
    "def convert_emojis(text: str, EMO_UNICODE: dict) -> str:\n",
    "    \"\"\"\n",
    "    Converts emojis to text in the input text.\n",
    "    \"\"\"\n",
    "    for description, emoji in EMO_UNICODE.items():\n",
    "        text = text.replace(emoji, \"_\".join(description.replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes URLs from the input text.\n",
    "    \"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_http_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes HTTP tags from the input text.\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(text, \"html.parser\").text\n",
    "\n",
    "def chat_words_conversion(text: str, slang_words_list: dict) -> str:\n",
    "    \"\"\"\n",
    "    Converts chat words to standard words in the input text.\n",
    "    \"\"\"\n",
    "    chat_words_list = list(slang_words_list.keys())\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        new_text.append(slang_words_list.get(word.upper(), word))\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def spell_correction(text: str, spell: SpellChecker) -> str:\n",
    "    \"\"\"\n",
    "    Corrects spelling errors in the input text.\n",
    "    \"\"\"\n",
    "    corrected_text = []\n",
    "    for word in text.split():\n",
    "        corrected_text.append(spell.correction(word))\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "remove_stopwords() missing 1 required positional argument: 'STOPWORDS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\antoi\\OneDrive\\Documents\\COURS_2023_S2\\Natural_Language_Processing\\TP1\\nlp_courses\\nlp_courses\\tp_1_text_cleaning\\bonus_notebook.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/OneDrive/Documents/COURS_2023_S2/Natural_Language_Processing/TP1/nlp_courses/nlp_courses/tp_1_text_cleaning/bonus_notebook.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39massert\u001b[39;00m lower_case(\u001b[39m\"\u001b[39m\u001b[39mHello World!\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhello world!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antoi/OneDrive/Documents/COURS_2023_S2/Natural_Language_Processing/TP1/nlp_courses/nlp_courses/tp_1_text_cleaning/bonus_notebook.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m remove_punctuation(\u001b[39m\"\u001b[39m\u001b[39mHello, World!\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHello World\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antoi/OneDrive/Documents/COURS_2023_S2/Natural_Language_Processing/TP1/nlp_courses/nlp_courses/tp_1_text_cleaning/bonus_notebook.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m remove_stopwords(\u001b[39m\"\u001b[39;49m\u001b[39mHello the World!\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWorld!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: remove_stopwords() missing 1 required positional argument: 'STOPWORDS'"
     ]
    }
   ],
   "source": [
    "# here we test our functions one by one\n",
    "assert lower_case(\"Hello World!\") == \"hello world!\"\n",
    "assert remove_punctuation(\"Hello, World!\") == \"Hello World\"\n",
    "assert remove_stopwords(\"Hello the World!\") == \"World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Chains all the cleaning functions together using scikit-learn pipelines.\n",
    "    \"\"\"\n",
    "    # Define your global variables (e.g., STOPWORDS, FREQWORDS, RAREWORDS) here\n",
    "\n",
    "    # Define the preprocessing steps as a list of tuples with (step_name, transformer_function)\n",
    "    preprocessing_steps = [\n",
    "        ('lower_case', FunctionTransformer(lower_case)),\n",
    "        ('remove_punctuation', FunctionTransformer(remove_punctuation)),\n",
    "        ('remove_stopwords', FunctionTransformer(lambda x: remove_stopwords(x, STOPWORDS))),\n",
    "        ('remove_frequent_words', FunctionTransformer(lambda x: remove_frequent_words(x, FREQWORDS))),\n",
    "        ('remove_rare_words', FunctionTransformer(lambda x: remove_rare_words(x, RAREWORDS))),\n",
    "        ('stemming', FunctionTransformer(lambda x: stemming(x, stemmer))),  # Replace 'stemmer' with your stemmer object\n",
    "        ('lemmatize', FunctionTransformer(lambda x: lemmatize(x, lemmatizer))),  # Replace 'lemmatizer' with your lemmatizer object\n",
    "        ('convert_emoticons', FunctionTransformer(lambda x: convert_emoticons(x, EMOTICONS))),\n",
    "        ('convert_emojis', FunctionTransformer(lambda x: convert_emojis(x, EMO_UNICODE))),\n",
    "        ('remove_urls', FunctionTransformer(remove_urls)),\n",
    "        ('remove_http_tags', FunctionTransformer(remove_http_tags)),\n",
    "        ('chat_words_conversion', FunctionTransformer(lambda x: chat_words_conversion(x, slang_words_list))),  # Replace 'slang_words_list' with your dictionary\n",
    "        ('spell_correction', FunctionTransformer(lambda x: spell_correction(x, spell)))  # Replace 'spell' with your SpellChecker object\n",
    "    ]\n",
    "\n",
    "    # Create the pipeline\n",
    "    preprocessing_pipeline = Pipeline(preprocessing_steps)\n",
    "\n",
    "    # Apply the pipeline to the input text\n",
    "    cleaned_text = preprocessing_pipeline.transform([text])[0]\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"nlp_courses/tp_1_text_cleaning/to_clean.csv\", index_col=0)\n",
    "    df[\"cleaned_text\"] = df.text.apply(lambda x: preprocessing_pipeline(x))\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\nBase text: {row.text}\")\n",
    "        print(f\"Cleaned text: {row.cleaned_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
